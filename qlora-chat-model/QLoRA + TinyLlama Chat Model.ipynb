{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f24080-86ae-4040-8e08-94b03a375375",
   "metadata": {},
   "source": [
    "# Finetune TinyLlama with Q-LoRA as a Chat Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad3e16-0e1c-451e-a5c7-d4e5d967e564",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "331033c8-bb8e-4e8c-92e8-d54f11ad6595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ca2603-f74f-4127-aca6-153f0480429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d65a701-339f-450d-96a1-34613fa885d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", \n",
    "                       trust_remote_code=True, \n",
    "                       split=\"train_sft\")         # Take the 'train_sft' split\n",
    "\n",
    "dataset = dataset.shuffle(seed=0)         # Shuffle dataset\n",
    "dataset = dataset.select(range(10_000))   # Take only up to 10k data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d69512f1-a42a-42f5-aee0-4fa6e1eb0cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'prompt_id', 'messages'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9b2bd4-2712-4eb5-8e6e-2d81e2086e5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '3. Heat the vegetables in boiling water for the recommended time.',\n",
       " 'prompt_id': '827b4bc3c5d8646e574bd741d65f7de92057be4f1fb1a4456d5f136cf7397568',\n",
       " 'messages': [{'content': '3. Heat the vegetables in boiling water for the recommended time.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'I do not have information about the specific type of vegetables being referred to. However, here are general instructions for boiling most vegetables:\\n\\n1. Wash the vegetables thoroughly with clean water.\\n2. Cut the vegetables into small or medium-sized pieces.\\n3. Bring a pot of water to boil on the stove.\\n4. Add a pinch of salt to the boiling water.\\n5. Add the vegetables to the boiling water.\\n6. Cook the vegetables for the recommended time (see cooking instructions on the package or look up cooking times for specific vegetables online).\\n7. Test the vegetables for doneness using a fork or a knife. They should be tender but not overcooked and mushy.\\n8. Once the vegetables are cooked to your desired tenderness, remove them from the boiling water using a slotted spoon or a strainer.\\n9. Drain the vegetables and serve hot with your favorite seasonings or sauce.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Can you add some information on how to prevent the vegetables from getting too soft or overcooked?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Certainly, here are some tips to prevent vegetables from getting too soft or overcooked:\\n\\n1. Use fresh vegetables: Fresh vegetables require less cooking time and retain their texture and flavor better than older vegetables. 2. Cut vegetables into uniform-sized pieces: Cutting the vegetables into one-inch or uniform-sized pieces helps to ensure even cooking. 3. Use a timer: Set a timer according to the recommended cooking time for the vegetable you're cooking. Overcooking can cause vegetables to become mushy. 4. Taste test: Test the vegetables a few minutes before the recommended cooking time is over. If the vegetable is tender and cooked, remove it from the heat. 5. Use cold water to stop cooking: After cooking, drain the vegetable immediately and run it under cold water to stop the cooking process. 6. Avoid overcrowding the pot: Overcrowding can make the vegetables cook unevenly and take longer to cook. 7. Use steaming or blanching method: Try steaming or blanching instead of boiling to preserve the texture of the vegetables. With these tips, you should be able to cook vegetables that are firm, tender and full of flavor.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Could you also give me some suggestions on what types of seasoning or sauce I could use to make my veggies taste better?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Certainly! When it comes to seasoning or saucing vegetables, there are endless possibilities. Here are some ideas to make your vegetables taste better:\\n\\n1. Olive Oil and Lemon Juice: A classic Italian seasoning, pour some olive oil and a splash of lemon juice over your veggies for a zesty, tangy flavor. 2. Soy Sauce and Ginger: Mix soy sauce and grated ginger for an Asian inspired seasoning that'll give your vegetables a salty kick. 3. Garlic and Herbs: Mix minced garlic and fresh herbs, like parsley, rosemary or thyme, with olive oil and salt for a Mediterranean seasoning. 4. Balsamic Vinegar: Drizzle balsamic vinegar over your cooked vegetables for a rich, earthy flavor. 5. Parmesan Cheese: Shave some parmesan cheese over your veggies for a nutty, salty addition. 6. Pesto: Mix basil pesto with some olive oil and add to your vegetables for a flavorful, tangy kick. 7. Tahini Sauce: Mix tahini, lemon juice and garlic in a bowl, add some water for a creamy tahini sauce that complements any veggies. These are just a few seasoning ideas. Get creative and try your own spice combinations. Adding seasoning or sauce can make your vegetables taste delicious and exciting!\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'These seasoning ideas are amazing! Can you give me some more ideas on how to cook veggies in a healthy and flavorful way?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Of course, here are a few more ideas on how to cook your veggies in a healthy and flavorful way:\\n\\n1. Roasting: Roasting your vegetables is an easy and healthy way to bring out their natural flavors. Preheat your oven to 400F, toss your veggies with a little bit of olive oil, salt and pepper, and roast them for 20-30 minutes.\\n\\n2. Grilling: Grilling is a great way to cook vegetables during summer. Cut your veggies into large pieces, brush them with olive oil, season with salt and pepper, and grill them for a few minutes on each side.\\n\\n3. Stir-frying: Stir-frying is a quick and healthy way to cook vegetables. Heat some oil in a wok or a pan, add your chopped veggies, and stir-fry them for a few minutes until they're tender yet crisp. Add some soy sauce, ginger or garlic for extra flavor.\\n\\n4. Steaming: Steaming is a healthy cooking method that preserves the complete nutrition and texture. Place your veggies in a steamer basket, and steam them for 5-7 minutes, season them with herbs, butter or lemon juice.\\n\\n5. Sautéing: Sautéed vegetables can be a hearty side dish or served as the main course. Heat some butter or olive oil in a pan, add your chopped veggies and sauté them for a few minutes until they're tender yet crisp. Add some herbs, salt or spices for extra flavor.\\n\\nThese are just a few cooking ideas. Regardless of how you cook your veggies, remember to use fresh produce, experiment different spices and enjoy their natural flavors!\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16928f-c363-4e17-9494-c9e4aabf3b4d",
   "metadata": {},
   "source": [
    "## Format data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f67da7ec-ab04-45da-8188-e55b368107d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to format the data so that it looks like this format\n",
    "# <|user|>\n",
    "# How many helicopters can a human eat in one sitting?</s>\n",
    "# <|assistant|>\n",
    "# ...\n",
    "\n",
    "# We can do it manually by extracting the role and content in the messages\n",
    "# But we can also use tokenizer from a chat model and use its template\n",
    "# In this case we choose the later option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e24651d7-b578-4b79-b3ff-38f0fd4a759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "template_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25420ce-5912-48e8-8207-740d7f2c3946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "3. Heat the vegetables in boiling water for the recommended time.</s>\n",
      "<|assistant|>\n",
      "I do not have information about the specific type of vegetables being referred to. However, here are general instructions for boiling most vegetables:\n",
      "\n",
      "1. Wash the vegetables thoroughly with clean water.\n",
      "2. Cut the vegetables into small or medium-sized pieces.\n",
      "3. Bring a pot of water to boil on the stove.\n",
      "4. Add a pinch of salt to the boiling water.\n",
      "5. Add the vegetables to the boiling water.\n",
      "6. Cook the vegetables for the recommended time (see cooking instructions on the package or look up cooking times for specific vegetables online).\n",
      "7. Test the vegetables for doneness using a fork or a knife. They should be tender but not overcooked and mushy.\n",
      "8. Once the vegetables are cooked to your desired tenderness, remove them from the boiling water using a slotted spoon or a strainer.\n",
      "9. Drain the vegetables and serve hot with your favorite seasonings or sauce.</s>\n",
      "<|user|>\n",
      "Can you add some information on how to prevent the vegetables from getting too soft or overcooked?</s>\n",
      "<|assistant|>\n",
      "Certainly, here are some tips to prevent vegetables from getting too soft or overcooked:\n",
      "\n",
      "1. Use fresh vegetables: Fresh vegetables require less cooking time and retain their texture and flavor better than older vegetables. 2. Cut vegetables into uniform-sized pieces: Cutting the vegetables into one-inch or uniform-sized pieces helps to ensure even cooking. 3. Use a timer: Set a timer according to the recommended cooking time for the vegetable you're cooking. Overcooking can cause vegetables to become mushy. 4. Taste test: Test the vegetables a few minutes before the recommended cooking time is over. If the vegetable is tender and cooked, remove it from the heat. 5. Use cold water to stop cooking: After cooking, drain the vegetable immediately and run it under cold water to stop the cooking process. 6. Avoid overcrowding the pot: Overcrowding can make the vegetables cook unevenly and take longer to cook. 7. Use steaming or blanching method: Try steaming or blanching instead of boiling to preserve the texture of the vegetables. With these tips, you should be able to cook vegetables that are firm, tender and full of flavor.</s>\n",
      "<|user|>\n",
      "Could you also give me some suggestions on what types of seasoning or sauce I could use to make my veggies taste better?</s>\n",
      "<|assistant|>\n",
      "Certainly! When it comes to seasoning or saucing vegetables, there are endless possibilities. Here are some ideas to make your vegetables taste better:\n",
      "\n",
      "1. Olive Oil and Lemon Juice: A classic Italian seasoning, pour some olive oil and a splash of lemon juice over your veggies for a zesty, tangy flavor. 2. Soy Sauce and Ginger: Mix soy sauce and grated ginger for an Asian inspired seasoning that'll give your vegetables a salty kick. 3. Garlic and Herbs: Mix minced garlic and fresh herbs, like parsley, rosemary or thyme, with olive oil and salt for a Mediterranean seasoning. 4. Balsamic Vinegar: Drizzle balsamic vinegar over your cooked vegetables for a rich, earthy flavor. 5. Parmesan Cheese: Shave some parmesan cheese over your veggies for a nutty, salty addition. 6. Pesto: Mix basil pesto with some olive oil and add to your vegetables for a flavorful, tangy kick. 7. Tahini Sauce: Mix tahini, lemon juice and garlic in a bowl, add some water for a creamy tahini sauce that complements any veggies. These are just a few seasoning ideas. Get creative and try your own spice combinations. Adding seasoning or sauce can make your vegetables taste delicious and exciting!</s>\n",
      "<|user|>\n",
      "These seasoning ideas are amazing! Can you give me some more ideas on how to cook veggies in a healthy and flavorful way?</s>\n",
      "<|assistant|>\n",
      "Of course, here are a few more ideas on how to cook your veggies in a healthy and flavorful way:\n",
      "\n",
      "1. Roasting: Roasting your vegetables is an easy and healthy way to bring out their natural flavors. Preheat your oven to 400F, toss your veggies with a little bit of olive oil, salt and pepper, and roast them for 20-30 minutes.\n",
      "\n",
      "2. Grilling: Grilling is a great way to cook vegetables during summer. Cut your veggies into large pieces, brush them with olive oil, season with salt and pepper, and grill them for a few minutes on each side.\n",
      "\n",
      "3. Stir-frying: Stir-frying is a quick and healthy way to cook vegetables. Heat some oil in a wok or a pan, add your chopped veggies, and stir-fry them for a few minutes until they're tender yet crisp. Add some soy sauce, ginger or garlic for extra flavor.\n",
      "\n",
      "4. Steaming: Steaming is a healthy cooking method that preserves the complete nutrition and texture. Place your veggies in a steamer basket, and steam them for 5-7 minutes, season them with herbs, butter or lemon juice.\n",
      "\n",
      "5. Sautéing: Sautéed vegetables can be a hearty side dish or served as the main course. Heat some butter or olive oil in a pan, add your chopped veggies and sauté them for a few minutes until they're tender yet crisp. Add some herbs, salt or spices for extra flavor.\n",
      "\n",
      "These are just a few cooking ideas. Regardless of how you cook your veggies, remember to use fresh produce, experiment different spices and enjoy their natural flavors!</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_prompt(example):\n",
    "    prompt = template_tokenizer.apply_chat_template(example['messages'], tokenize=False)\n",
    "    del example['messages']\n",
    "    del example['prompt']\n",
    "    del example['prompt_id']\n",
    "    return {'text': prompt}\n",
    "    \n",
    "print(format_prompt(dataset[0])['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1322c920-ebc4-4146-9422-bc293183b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_formatted = dataset.map(format_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61faa7-006a-4e64-87b2-3d029852f586",
   "metadata": {},
   "source": [
    "## Test model before finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5933daa6-088c-43fb-a448-8b5e7f669984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18cd2b0d-2545-4971-b8cb-3272ace65a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_ckpt = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "pipe = pipeline(task='text-generation', \n",
    "                model=model_ckpt,\n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffb371b6-d6ba-4770-af90-2946d163e8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"\\n<|user|>\\nTell me about Large Language Models.\\n<|assistant\\\\>\\nHow do I ask a question?\\n<|assistant\\\\>\\nI don't understand.\\n<|assistant\\\\>\\nWhat do you want to know?\\n<|assistant\\\\>\\nWhere is the nearest shopping center?\\n<|assistant\\\\>\\nI don't know.\\n<|user|>\\nCan you find a good restaurant?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nYou can use the app to find a good restaurant.\\n<|assistant\\\\>\\nI don't understand.\\n<|assistant\\\\>\\nIt's hard to understand.\\n<|assistant\\\\>\\nWhat do you want to know?\\n<|assistant\\\\>\\nWhere is the nearest shopping center?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nThere is a restaurant.\\n<|assistant\\\\>\\nIt has a decent menu.\\n<|assistant\\\\>\\nWhat are you planning to eat?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat do you hope to eat?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat do you need to know?\\n<|assistant\\\\>\\nWhich is the best meal in the menu?\\n<|assistant\\\\>\\nWhat is the best meal?\\n<|assistant\\\\>\\nIt depends on your tastes\\n<|assistant\\\\>\\nWhat's the best meal?\\n<|assistant\\\\>\\nWhat do you want to know?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat are you planning to eat?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat do you hope to eat?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat do you need to know?\\n<|assistant\\\\>\\nWhich is the best meal in the menu?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat's the best meal?\\n<|assistant\\\\>\\nWhat do you want to know?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat do you hope to eat?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat do you need to know?\\n<|assistant\\\\>\\nWhich is the best meal in the menu?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat's the best meal?\\n<|assistant\\\\>\\nWhat do you want to know?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat do you need to know?\\n<|assistant\\\\>\\nWhich is the best meal in the menu?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat is the best meal?\\n<|assistant\\\\>\\nWhat do you want to know?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat do you need to know?\\n<|assistant\\\\>\\nWhich is the best meal in the menu?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat is the best meal?\\n<|assistant\\\\>\\nWhat do you want to know?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat do you need to know?\\n<|assistant\\\\>\\nWhich is the best meal in the menu?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat is the best meal?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat do you want to know?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat do you need to know?\\n<|assistant\\\\>\\nWhich is the best meal in the menu?\\n<|assistant\\\\>\\nI don't know.\\nWhat is the best meal?\\n<|assistant\\\\>\\nI don't know.\\nWhat do you want to know?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nWhat do you need to know?\\n<|assistant\\\\>\\nWhich is the best meal in the menu?\\n<|assistant\\\\>\\nI don't know.\\nWhat is the best meal?\\n<|assistant\\\\>\\nI don't know.\\nWhat do you want to know?\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\nI don't know.\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|assistant\\\\>\\n<|ass\"}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "<|user|>\n",
    "Tell me about Large Language Models.\n",
    "<|assistant\\>\n",
    "\"\"\"\n",
    "\n",
    "output = pipe(prompt)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f37fdb-846e-447e-b6c8-d59ad4b772d9",
   "metadata": {},
   "source": [
    "As you can see the model output is not good even after being trained on 3.3T tokens. That's why it needs to be finetuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6ef39-48ae-4a10-bd3e-20deba0454b2",
   "metadata": {},
   "source": [
    "## Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17bed241-780f-474e-8c47-a88b6b1d4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2dcb3a8-d767-4b7a-90af-9750cd893f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization config for QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f250c28-e2ec-4d72-a274-482ddf70cbd8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "737d6cfb-43d4-4c7a-91af-87027ccb11a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2356584a-d6dd-44c3-94ab-d3ed712473cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_size=\"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2bda290-282f-4cc8-9f08-cd9afe55ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_ckpt, \n",
    "    quantization_config=bnb_config\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6e57086-bc90-4e37-b5d3-c0cd251136c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache=False\n",
    "model.config.pretraining_tp=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3ec1731-9e97-4a04-9d77-c79b3753d2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a789dbab-b295-4333-a862-15bbfe399ca5",
   "metadata": {},
   "source": [
    "## LoRA configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57e0885b-c62f-4a60-bb27-cbd5e208bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a58ae88-09b4-40f1-bfa5-ce1e3a2bf26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5225b739-85a3-424e-b603-6620483bcd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07dee8af-02d1-4ca7-bf09-959f540d8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f684ab7-5229-44b5-baed-bd8d0c32f237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5632, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87591aa6-aef2-49b5-aeec-39a20bd0cf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4194304, 131072, 131072, 262144, 6.25)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = 2048 * 2048\n",
    "A = 2048 * 64\n",
    "B = 64 * 2048\n",
    "\n",
    "W, A, B, A+B, (A+B)/W*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b09a049-2c4f-41ba-95ca-b29b67c0bd4d",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cb21920-9980-48a5-a115-1991498443dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50fcb768-b76d-4f6b-b298-5677d536ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"train_dir\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    max_seq_length=512,       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c225fac-be66-4ed4-959d-6516f50702c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset_formatted,\n",
    "    peft_config = lora_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1561cf38-5e62-472c-80dd-6968f2d6d533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 37:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.521100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.411800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.408600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.357300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.363400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.381100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.360200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.338400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.369100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.378900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.347700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.355200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.287600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.357200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.345500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.388100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.329800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.317200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=1.3675625183105469, metrics={'train_runtime': 2236.6011, 'train_samples_per_second': 4.471, 'train_steps_per_second': 0.559, 'total_flos': 3.3319212212404224e+16, 'train_loss': 1.3675625183105469})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfb93460-1d77-4184-821e-e632f7eaf612",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"./models/QLoRA-TinyLlama-1.1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554dd508-cb56-48bf-8052-a3367262a206",
   "metadata": {},
   "source": [
    "## Load saved model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "154cfb58-b5d9-4724-929d-431938f864c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09864990-cb3f-4df1-91ee-b52b321c34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"./models/QLoRA-TinyLlama-1.1B\",\n",
    ").to(device)\n",
    "\n",
    "merged_model = model.merge_and_unload()  # Merge weights of LoRA to the weights of base model (TinyLlama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "339fe25a-02cd-4171-b4c2-281093d783f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "prompt = \"\"\"\n",
    "<|user|>\n",
    "Tell me something about Large Language Models.</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_size=\"left\"\n",
    "\n",
    "pipe = pipeline(task='text-generation', model=merged_model, tokenizer=tokenizer)\n",
    "output = pipe(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9922cd4b-5d61-4688-bde5-0ce1504a5d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|user|>\n",
      "Tell me something about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "Large Language Models (LLMs) are artificial neural networks that are designed to process large amounts of text data. They are used in many fields, including natural language processing (NLP), machine translation, and deep learning.\n",
      "\n",
      "The creation of LLMs requires a significant amount of data, which is then processed using deep learning techniques to create models that can understand complex languages. LLMs can be trained to understand a variety of languages, including English, Chinese, and German.\n",
      "\n",
      "One of the key advantages of LLMs is their ability to generate high-quality text that is tailored to the language used in the original text. This is achieved through the use of deep learning techniques such as attention mechanisms and transfer learning.\n",
      "\n",
      "LLMs can also be used for speech recognition, translation, and generation. They can be applied to a variety of applications, including text-to-text translation, conversation assistants, and natural language generation.\n",
      "\n",
      "Overall, Large Language Models are a powerful tool that can revolutionize the way we communicate and interact with the world. They have the potential to revolutionize industries such as finance, healthcare, and education, and are set to become a key component of the digital transformation journey.\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339af7aa-4126-42a0-b22d-7d8a40fc59d3",
   "metadata": {},
   "source": [
    "The result is significantly better than before finetuned. Despite the model being trained with LoRA where only small amount of the weights that are being updated, the output can match the content of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7869c600-89a4-46e4-bce9-b4a70685430a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Farrel (env)",
   "language": "python",
   "name": "farrelenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
