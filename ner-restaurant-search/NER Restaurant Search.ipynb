{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd4d9b8e-b626-46a0-9100-5d3587b58e67",
   "metadata": {},
   "source": [
    "# NER Restaurant Search with TinyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07143a3-dd8e-49e1-8172-d2e1494318b2",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f5f73e8-fcfa-41da-983f-688ff55b3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bed375-c669-4030-93d2-8d4e89e90b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b79c19a-22d7-40a2-8cbd-ac810b8852b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdee556-bf41-4574-82f9-7cd5fd985b1d",
   "metadata": {},
   "source": [
    "The dataset is from these links. The first one will be used on this project.\n",
    "- https://groups.csail.mit.edu/sls/downloads/restaurant/\n",
    "- https://huggingface.co/datasets/tner/mit_restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8653ed7-33c2-4f56-96de-679f1261c221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Rating</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-Rating</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O</td>\n",
       "      <td>restaurants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-Amenity</td>\n",
       "      <td>inside</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0            1\n",
       "0   B-Rating            2\n",
       "1   I-Rating        start\n",
       "2          O  restaurants\n",
       "3          O         with\n",
       "4  B-Amenity       inside"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/ner_train.bio\", sep=\"\\t\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3466813d-02f6-4d28-a16a-6de39c416fa3",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb05cc4e-8758-4d00-b1aa-279a8469f705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Rating\\t2\\n', 'I-Rating\\tstart\\n']\n",
      "['B-Rating\\t2', 'I-Rating\\tstart']\n"
     ]
    }
   ],
   "source": [
    "with open(\"datasets/ner_train.bio\", \"r\") as f:\n",
    "    texts = f.readlines()\n",
    "print(texts[:2])\n",
    "\n",
    "# Remove new line char\n",
    "texts = [text.replace(\"\\n\", \"\") for text in texts]\n",
    "print(texts[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cedbd6-bc01-44b6-a808-552e6b5ab0ac",
   "metadata": {},
   "source": [
    "We want to make the format become something like this:\n",
    "```\n",
    "{\n",
    "    'tags': [0, 0, 0, 0, 0, 0, 0, 0, 5, 3, 4, 0],\n",
    "    'tokens': ['can', 'you', 'find', 'the', 'phone', 'number', 'for', 'the', 'closest', 'family', 'style', 'restaurant']\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234ac0fe-e4c4-45d3-87c9-f8833c46a6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-Rating\\t5',\n",
       " 'I-Rating\\tstar',\n",
       " 'O\\tresturants',\n",
       " 'B-Location\\tin',\n",
       " 'I-Location\\tmy',\n",
       " 'I-Location\\ttown',\n",
       " '',\n",
       " 'O\\t98',\n",
       " 'B-Restaurant_Name\\thong',\n",
       " 'I-Restaurant_Name\\tkong',\n",
       " 'O\\trestaurant']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[9:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf22f8d-24d3-468c-8aef-899890a269f5",
   "metadata": {},
   "source": [
    "Notice that if the line is empty, then the next line is a new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fffa4264-511d-4899-bc8b-bfcbc41165e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags = []\n",
    "train_tokens = []\n",
    "\n",
    "temp_tags = []\n",
    "temp_tokens = []\n",
    "for line in texts:\n",
    "    if line != \"\":\n",
    "        tag, token = line.split('\\t')\n",
    "        temp_tags.append(tag)\n",
    "        temp_tokens.append(token)\n",
    "    else:\n",
    "        train_tags.append(temp_tags)\n",
    "        train_tokens.append(temp_tokens)\n",
    "        temp_tags, temp_tokens = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0def18d8-336f-4113-9b69-8c67e6207425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O\\ta\\n', 'B-Rating\\tfour\\n']\n",
      "['O\\ta', 'B-Rating\\tfour']\n"
     ]
    }
   ],
   "source": [
    "with open(\"datasets/ner_test.bio\", \"r\") as f:\n",
    "    texts = f.readlines()\n",
    "print(texts[:2])\n",
    "\n",
    "# Remove new line char\n",
    "texts = [text.replace(\"\\n\", \"\") for text in texts]\n",
    "print(texts[:2])\n",
    "\n",
    "test_tags = []\n",
    "test_tokens = []\n",
    "\n",
    "temp_tags = []\n",
    "temp_tokens = []\n",
    "for line in texts:\n",
    "    if line != \"\":\n",
    "        tag, token = line.split('\\t')\n",
    "        temp_tags.append(tag)\n",
    "        temp_tokens.append(token)\n",
    "    else:\n",
    "        test_tags.append(temp_tags)\n",
    "        test_tokens.append(temp_tokens)\n",
    "        temp_tags, temp_tokens = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8866ef2-92d2-448a-aaab-d44fb6b832fc",
   "metadata": {},
   "source": [
    "### Split train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "360e9f3d-5028-41a4-a435-700cdbdbb588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "total_val = math.floor(len(test_tokens) / 3)\n",
    "total_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc7fb0c2-5203-415f-adba-1dff1967e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_tags = test_tags[:total_val]\n",
    "validation_tokens = test_tokens[:total_val]\n",
    "test_tags = test_tags[total_val:]\n",
    "test_tokens = test_tokens[total_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4345bc8a-1b44-436b-94d6-5946af5c7fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7659, 7659, 506, 506, 1014, 1014)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tags), len(train_tokens), len(validation_tags), len(validation_tokens), len(test_tags), len(test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98f80b-bac3-478d-86f1-f7f5c5e64f0a",
   "metadata": {},
   "source": [
    "### Prepare datasetdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e535d068-aa2c-4c6a-8878-9d9f9e342c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'tokens': train_tokens, 'tags_str': train_tags})\n",
    "df_validation = pd.DataFrame({'tokens': validation_tokens, 'tags_str': validation_tags})\n",
    "df_test = pd.DataFrame({'tokens': test_tokens, 'tags_str': test_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6a0d739-29ed-4089-ac3b-c565d1f443b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61a8189c-c63b-4445-a786-46e6678425c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'tags_str'],\n",
       "        num_rows: 7659\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'tags_str'],\n",
       "        num_rows: 506\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'tags_str'],\n",
       "        num_rows: 1014\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict(\n",
    "    {\n",
    "        'train': Dataset.from_pandas(df_train, preserve_index=False),\n",
    "        'validation': Dataset.from_pandas(df_validation, preserve_index=False),\n",
    "        'test': Dataset.from_pandas(df_test, preserve_index=False)\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e0e2adc-c1c4-4854-992b-692fd738d199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['2', 'start', 'restaurants', 'with', 'inside', 'dining'],\n",
       " 'tags_str': ['B-Rating', 'I-Rating', 'O', 'O', 'B-Amenity', 'I-Amenity']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e877c10-e008-4d41-b470-1e1e69672c73",
   "metadata": {},
   "source": [
    "### Prepare tag2index and index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56abd191-f711-4e7e-ad61-ee7fa5144ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set()\n",
    "for tags in dataset['train']['tags_str']:\n",
    "    unique_tags.update(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1fe0bb5-97c7-4174-8730-e3ab077cc38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hours',\n",
       " 'Rating',\n",
       " 'Price',\n",
       " 'Restaurant_Name',\n",
       " 'Location',\n",
       " 'Dish',\n",
       " 'Amenity',\n",
       " 'Cuisine']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tags = list(set(tag.split('-')[1] for tag in unique_tags if tag != 'O'))\n",
    "unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d0015f2-2d34-4d1c-8b2a-932f51f62e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-Hours': 1, 'I-Hours': 2, 'B-Rating': 3, 'I-Rating': 4, 'B-Price': 5, 'I-Price': 6, 'B-Restaurant_Name': 7, 'I-Restaurant_Name': 8, 'B-Location': 9, 'I-Location': 10, 'B-Dish': 11, 'I-Dish': 12, 'B-Amenity': 13, 'I-Amenity': 14, 'B-Cuisine': 15, 'I-Cuisine': 16}\n"
     ]
    }
   ],
   "source": [
    "tag2index = {\"O\": 0}\n",
    "for i, tag in enumerate(unique_tags):\n",
    "    tag2index[f\"B-{tag}\"] = len(tag2index)\n",
    "    tag2index[f\"I-{tag}\"] = len(tag2index)\n",
    "\n",
    "index2tag = {v: k for k, v in tag2index.items()}\n",
    "print(tag2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910dc52-22ac-4fc4-864d-330c564ff216",
   "metadata": {},
   "source": [
    "### Map dataset with tag2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dd7bd7b-bd0b-4c2f-871c-d6cad8fa44c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7659/7659 [00:00<00:00, 15755.84 examples/s]\n",
      "Map: 100%|██████████| 506/506 [00:00<00:00, 13430.86 examples/s]\n",
      "Map: 100%|██████████| 1014/1014 [00:00<00:00, 16336.42 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'tags_str', 'tags'],\n",
       "        num_rows: 7659\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'tags_str', 'tags'],\n",
       "        num_rows: 506\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'tags_str', 'tags'],\n",
       "        num_rows: 1014\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda example: {\"tags\": [tag2index[tag] for tag in example['tags_str']]})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e309ae7f-2ad3-4501-8af3-73e85ebabf8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['2', 'start', 'restaurants', 'with', 'inside', 'dining'],\n",
       " 'tags_str': ['B-Rating', 'I-Rating', 'O', 'O', 'B-Amenity', 'I-Amenity'],\n",
       " 'tags': [3, 4, 0, 0, 13, 14]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba77a04-5f9a-4a92-a2ec-4eb09eb6da5a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b60ec-5846-466a-8063-9add56a1451b",
   "metadata": {},
   "source": [
    "### Tokenization and label alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c50cc78-0672-4d5e-8b11-2ece3100338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ccfef09-5783-443e-af37-262fa96cc36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ckpt = \"distilbert/distilbert-base-uncased\"\n",
    "model_ckpt = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2851c081-3016-4a5c-9f3c-0dac6016e5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5', 'star', 'resturants', 'in', 'my', 'town']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = dataset['train'][2]['tokens']\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffa0451b-a97a-4913-b1ff-95a58acf46ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1019, 2732, 2717, 4648, 7666, 1999, 2026, 2237, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer(input, is_split_into_words=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab862007-7e50-45fb-83d1-09a517e0cf48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 2, 2, 3, 4, 5, None]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "472360f1-a2e7-42a7-81eb-29b8820098de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '5', 'star', 'rest', '##ura', '##nts', 'in', 'my', 'town', '[SEP]']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(output.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d64b86-4c61-409f-8533-524abeab60fa",
   "metadata": {},
   "source": [
    "This where the problem emerges. The words that already been tagged are splitted into multiple tokens. That means, all the tokens related to the same word must also have the same tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1444f70-cb4e-4291-a4a1-0e7480524214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True, max_length=512)\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "    \n",
    "        previous_word_id = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100) # -100 means it won't be included in loss calculation\n",
    "            elif word_id != previous_word_id:\n",
    "                label_ids.append(label[word_id])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_id = word_id\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81bafb2e-16dc-4950-a952-cc992d7c87bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7659/7659 [00:00<00:00, 10285.81 examples/s]\n",
      "Map: 100%|██████████| 506/506 [00:00<00:00, 12412.81 examples/s]\n",
      "Map: 100%|██████████| 1014/1014 [00:00<00:00, 13816.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c1ea9-401e-450a-b357-d59194fc7172",
   "metadata": {},
   "source": [
    "### Data collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baac6c18-3946-4a27-a324-9c2f97eb306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82ee7a8d-a9c5-4e30-98f9-7e181bbd401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd98e09d-d91e-4e4a-9d9b-d0c3d4d871aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fa734d7-53d8-40f6-a3e2-f30f71d852d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-Hours',\n",
       " 'I-Hours',\n",
       " 'B-Rating',\n",
       " 'I-Rating',\n",
       " 'B-Price',\n",
       " 'I-Price',\n",
       " 'B-Restaurant_Name',\n",
       " 'I-Restaurant_Name',\n",
       " 'B-Location',\n",
       " 'I-Location',\n",
       " 'B-Dish',\n",
       " 'I-Dish',\n",
       " 'B-Amenity',\n",
       " 'I-Amenity',\n",
       " 'B-Cuisine',\n",
       " 'I-Cuisine']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = evaluate.load('seqeval')\n",
    "label_names = list(tag2index)\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4736937-0977-4968-9af8-7580e52d9529",
   "metadata": {},
   "source": [
    "### Define compute metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09fe4dbd-b0aa-4bfe-baa8-d30bab336cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(evaluation_preds):\n",
    "    logits, labels = evaluation_preds\n",
    "\n",
    "    # Exclude the -100 labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] \n",
    "                   for label in labels]\n",
    "    \n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Exclude the -100\n",
    "    true_predictions = [[label_names[p] for p, l in zip(prediction, label) if l != -100] \n",
    "                        for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    metrics = metric.compute(predictions=true_predictions, \n",
    "                             references=true_labels)\n",
    "\n",
    "    return {\n",
    "        \"precision\": metrics['overall_precision'],\n",
    "        \"recall\": metrics['overall_recall'],\n",
    "        \"f1\": metrics['overall_f1'],\n",
    "        \"accuracy\": metrics['overall_accuracy'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27db314-7656-4751-8042-f47b7fdce394",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1cf60fd-87b6-47a7-814b-04dfba17024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11b12456-72a4-46a7-af64-7ddec1a9f073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huawei-noah/TinyBERT_General_4L_312D'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b21c30c-e2a5-44ad-a122-f5294d0830b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = AutoConfig.from_pretrained(model_ckpt, label2id=tag2index, id2label=index2tag)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_ckpt, config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af55b0d0-2819-4b37-a0ba-8608796ffcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f02bf2de-42b6-4d33-b76c-96793d7c4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='train_dir',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    disable_tqdm=False,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0a951a9-7f32-4ff6-af18-c7da483439f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ac14864-fb1e-4f6b-ba6e-5256ae421ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args, \n",
    "                  compute_metrics=compute_metrics, \n",
    "                  train_dataset=tokenized_dataset['train'], \n",
    "                  eval_dataset=tokenized_dataset['validation'], \n",
    "                  data_collator=data_collator,\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca4b19a7-d356-4ec8-aba2-8733643b0f40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4790' max='4790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4790/4790 02:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.587155</td>\n",
       "      <td>0.585406</td>\n",
       "      <td>0.664783</td>\n",
       "      <td>0.622575</td>\n",
       "      <td>0.859553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.536700</td>\n",
       "      <td>0.447729</td>\n",
       "      <td>0.650847</td>\n",
       "      <td>0.723164</td>\n",
       "      <td>0.685103</td>\n",
       "      <td>0.885913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.427300</td>\n",
       "      <td>0.404118</td>\n",
       "      <td>0.681661</td>\n",
       "      <td>0.741996</td>\n",
       "      <td>0.710550</td>\n",
       "      <td>0.895825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>0.387807</td>\n",
       "      <td>0.679422</td>\n",
       "      <td>0.752354</td>\n",
       "      <td>0.714030</td>\n",
       "      <td>0.897933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.358100</td>\n",
       "      <td>0.377438</td>\n",
       "      <td>0.698785</td>\n",
       "      <td>0.758004</td>\n",
       "      <td>0.727191</td>\n",
       "      <td>0.900886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4790, training_loss=0.5004753797686423, metrics={'train_runtime': 163.4738, 'train_samples_per_second': 234.258, 'train_steps_per_second': 29.301, 'total_flos': 18846679290180.0, 'train_loss': 0.5004753797686423, 'epoch': 5.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cba74f93-632f-428f-84b3-36e6bb55fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./models/ner_tinybert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a759a892-c9d8-4513-9429-7a0d2c74f354",
   "metadata": {},
   "source": [
    "### Load model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69d65fa2-2c44-4956-8d83-6e08f73abfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('token-classification', model='models/ner_tinybert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9327ccb9-ba90-4962-a500-d9e5c8298925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-Rating',\n",
       "  'score': np.float32(0.8767938),\n",
       "  'index': 5,\n",
       "  'word': 'best',\n",
       "  'start': 28,\n",
       "  'end': 32},\n",
       " {'entity': 'B-Dish',\n",
       "  'score': np.float32(0.7074773),\n",
       "  'index': 6,\n",
       "  'word': 'shu',\n",
       "  'start': 33,\n",
       "  'end': 36},\n",
       " {'entity': 'B-Dish',\n",
       "  'score': np.float32(0.40014237),\n",
       "  'index': 7,\n",
       "  'word': '##shi',\n",
       "  'start': 36,\n",
       "  'end': 39},\n",
       " {'entity': 'B-Location',\n",
       "  'score': np.float32(0.7823262),\n",
       "  'index': 9,\n",
       "  'word': 'new',\n",
       "  'start': 43,\n",
       "  'end': 46},\n",
       " {'entity': 'I-Location',\n",
       "  'score': np.float32(0.91048986),\n",
       "  'index': 10,\n",
       "  'word': 'york',\n",
       "  'start': 47,\n",
       "  'end': 51}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"which restaurant serves the best shushi in new york?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a663c60-cd46-4be3-9b7f-bc20cd17d0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Farrel (env)",
   "language": "python",
   "name": "farrelenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
